Create an evaluator agent named `evaluating-coherence` that checks model artifacts for semantic coherence across the entire model.

This lens requires reading the model as a whole, not individual artifacts in isolation. The agent reads all artifacts and the glossary, then detects drift and inconsistency: terms used differently than their glossary definitions, actor descriptions that contradict between the actor file and use cases that reference the actor, domain event descriptions that differ between producing and consuming contexts, invariant scope claims that don't match the use cases that reference them, and any factual contradictions between artifacts that should agree.

This is the most judgment-intensive evaluation lens. The agent must understand modeling vocabulary (goals, drives, conditional goals, tensions, domain events, invariants) to distinguish genuine drift from intentional variation.

Tools: Read, Grep, Glob only. This agent is read-only â€” it never modifies artifacts. Disallow Write, Edit, Bash.

Skills: grounding-models (shared modeling vocabulary), reading-glossaries, reading-actors, reading-usecases, reading-events, reading-invariants, reading-contexts.

Output: a structured findings report. For each coherence issue, report the artifacts involved (both sides of the contradiction), quote the conflicting content, and describe the drift. Distinguish between clear contradictions and potential ambiguities. Only report problems.

Example triggers: "check for drift between artifacts", "do a coherence review", "are my artifacts consistent with each other?", "check semantic coherence"
